#Assignment write-up

Note to the graders: I apologize for not creating the HTML version as recommended.  I am clearly not a GitHub expert.

#Summary:
 	Used desicion tree and random forest models to train.  
 	Out of sample error rates were 50% and 1% respectively.
 	Trained on 30% of the data, and tested on the remaining 70%.
 	Please read full write up for more detail.

#Preprocessing: 
I first turned Classe into a factor to aid with the modeling.  I noticed that when the new_window variable was NO, 
most fields are missing.  Also, in the small dataset which we are using for validation, new_window  is NO for all 
records, and therefore I decided that all the missing fields are can be ignored for this project.  I believe 
training the model with these fields in the data would still be potentially useful for removing missing-variable 
bias and improving the estimates of the fields that were actually populated, however those missing fields only 
make up about 2% or the data, and given the resource constraints on my small laptop, I decided that modeling 
those variables to gain a small amount of accuracy would not be worth the effort.  I also double checked the 
distribution of the classe variable compared to new_window (using a simple one-way test) and determined that 
these missing values do not seem explain the classe variable: that is, the distribution of classe is almost 
identical for new_window  = NO as is it for new_window  = YES. 

I also noticed the the index column was perfectly correlated with the classe variable in the training set.  
I eliminated this variable since I assumed that this could not be used for prediction on the validation set.

#Model training:
I used the train function within the caret package to train the model.  Since the target variable is 
categorical with 5 levels, a decision tree or random forest seemed the most appropriate.  I attempted to 
use a random forest but my PC could not handle the processing requirements.  I settle on using a decision 
tree, since it was much less computationally demanding, but it produced an expected out of sample error 
rate of about 50%.  Eventually I learned of a few ways to make the random forest run more easily, and 
was able to produce a model with an expected out of sample error rate of about 1%.  They key modifications
were 1) reduce the training set size down siginificantly; this still produced a great model but with much
less demand on my PC.  2) After enough research I discovered a few parameters to adjust within the train
function that allowed the model to train faster.

#Validation:
Before training the model, I created a training set consisting of 30% of the original modeling data and 
held out 70% for testing the model.  I originally started with a 75/25 split for the training/holdout, but 
could not run a random forest using this much data.  I set the K-fold cross-validation to 5 in the train
 function, and ran the model.  Using the confusion matrix function, I calculated the accuracy level to be
 99% (which is an OOS error rate = 1%). This was far more than what I achieved using a simple decision tree.
 The accuracy was enough to earn 100% correct on the validation set, so I saw no need to test more models.
